@misc{chen2020interpretable,
    abstract = {Unlike popular modularized framework, end-to-end autonomous driving seeks to solve the perception, decision and control problems in an integrated way, which can be more adapting to new scenarios and easier to generalize at scale. However, existing end-to-end approaches are often lack of interpretability, and can only deal with simple driving tasks like lane keeping. In this paper, we propose an interpretable deep reinforcement learning method for end-to-end autonomous driving, which is able to handle complex urban scenarios. A sequential latent environment model is introduced and learned jointly with the reinforcement learning process. With this latent model, a semantic birdeye mask can be generated, which is enforced to connect with a certain intermediate property in today's modularized framework for the purpose of explaining the behaviors of learned policy. The latent space also significantly reduces the sample complexity of reinforcement learning. Comparison tests with a simulated autonomous car in CARLA show that the performance of our method in urban scenarios with crowded surrounding vehicles dominates many baselines including DQN, DDPG, TD3 and SAC. Moreover, through masked outputs, the learned policy is able to provide a better explanation of how the car reasons about the driving environment. The codes and videos of this work are available at our github repo and project website.},
    author = {Jianyu Chen and Shengbo Eben Li and Masayoshi Tomizuka},
    keywords = {Deep reinforcement learn-ing,End-to-end driving policy,Index Terms-Autonomous driving,Interpretability,Probabilistic graphical model},
    month = {1},
    title = {Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning},
    url = {http://arxiv.org/abs/2001.08726},
    year = {2020},
}

@article{peng2021end,
    abstract = {<p> Recent years have seen the rapid development of autonomous driving systems, which are typically designed in a hierarchical architecture or an end-to-end architecture. The hierarchical architecture is always complicated and hard to design, while the end-to-end architecture is more promising due to its simple structure. This paper puts forward an end-to-end autonomous driving method through a deep reinforcement learning algorithm Dueling Double Deep Q-Network, making it possible for the vehicle to learn end-to-end driving by itself. This paper firstly proposes an architecture for the end-to-end lane-keeping task. Unlike the traditional image-only state space, the presented state space is composed of both camera images and vehicle motion information. Then corresponding dueling neural network structure is introduced, which reduces the variance and improves sampling efficiency. Thirdly, the proposed method is applied to The Open Racing Car Simulator (TORCS) to demonstrate its great performance, where it surpasses human drivers. Finally, the saliency map of the neural network is visualized, which indicates the trained network drives by observing the lane lines. A video for the presented work is available online, <ext-link ext-link-type="uri" href="https://youtu.be/76ciJmIHMD8">https://youtu.be/76ciJmIHMD8</ext-link> or <ext-link ext-link-type="uri" href="https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html">https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html</ext-link> . </p>},
    author = {Baiyu Peng and Qi Sun and Shengbo Eben Li and Dongsuk Kum and Yuming Yin and Junqing Wei and Tianyu Gu},
    doi = {10.1007/s42154-021-00151-3},
    issn = {2096-4250},
    issue = {3},
    journal = {Automotive Innovation},
    keywords = {Deep Q-network,End-to-end autonomous driving,Neural network,Reinforcement learning},
    month = {8},
    pages = {328-337},
    publisher = {Springer Science and Business Media B.V.},
    title = {End-to-End Autonomous Driving Through Dueling Double Deep Q-Network},
    volume = {4},
    url = {https://link.springer.com/10.1007/s42154-021-00151-3},
    year = {2021},
}

@misc{capasso2021endtoend,
    title = {End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning},
    author = {A. P. Capasso and P. Maramotti and A. Dell’Eva and A. Broggi},
    year = {2021},
    eprint = {2104.13617},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

@misc{xu2016endtoend,
    title = {End-to-end Learning of Driving Models from Large-scale Video Datasets},
    author = {H. Xu and Y. Gao and F. Yu and T. Darrell},
    year = {2016},
    eprint = {1612.01079},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}

@misc{toromanoff2019endtoend,
    title = {End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances},
    author = {M. Toromanoff and E. Wirbel and F. Moutarde},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1911.10868},
    primaryClass = {cs.RO}
}

@misc{mnih2016asynchronous,
    title = {Asynchronous Methods for Deep Reinforcement Learning},
    author = {V. Mnih and et al.},
    year = {2016},
    eprint = {1602.01783},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{schulman2017proximal,
    title = {Proximal Policy Optimization Algorithms},
    author = {J. Schulman and F. Wolski and P. Dhariwal and A. Radford and O. Klimov},
    year = {2017},
    eprint = {1707.06347},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{schulman2015trust,
    title = {Trust Region Policy Optimization},
    author = {J. Schulman and S. Levine and P. Moritz and M. I. Jordan and P. Abbeel},
    year = {2015},
    eprint = {1502.05477},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{peng2021learning,
    title = {Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization},
    author = {Z. Peng and Q. Li and K. M. Hui and C. Liu and B. Zhou},
    year = {2021},
    eprint = {2110.13827},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

@misc{yang2018mean,
    title = {Mean Field Multi-Agent Reinforcement Learning},
    author = {Y. Yang and R. Luo and M. Li and M. Zhou and W. Zhang and J. Wang},
    year = {2018},
    eprint = {1802.05438},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{terry2020pettingzoo,
    title = {PettingZoo: Gym for Multi-Agent Reinforcement Learning},
    author = {J. K. Terry and et al.},
    year = {2020},
    eprint = {2009.14471},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@article{wu2020cooperative,
    title = {Cooperative Multiagent Deep Deterministic Policy Gradient (CoMADDPG) for Intelligent Connected Transportation with Unsignalized Intersection},
    author = {T. Wu and M. Jiang and L. Zhang},
    journal = {Math Probl Eng},
    volume = {2020},
    year = {2020},
    doi = {10.1155/2020/1820527}
}

@misc{petrazzini2021proximal,
    title = {Proximal Policy Optimization with Continuous Bounded Action Space via the Beta Distribution},
    author = {I. G. B. Petrazzini and E. A. Antonelo},
    year = {2021},
    eprint = {2111.02202},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{lee2021investigating,
    title = {Investigating the state of connected and autonomous vehicles: A literature Review},
    author = {K. M. Lee and S. G. Subramanian and M. Crowley},
    year = {2021},
    eprint = {2111.01100},
    archivePrefix = {arXiv},
    primaryClass = {cs.RO}
}

@article{yang2022inductive,
    title = {An inductive heterogeneous graph attention-based multi-agent deep graph infomax algorithm for adaptive traffic signal control},
    author = {S. Yang and B. Yang},
    journal = {Information Fusion},
    volume = {88},
    pages = {249-262},
    year = {2022},
    month = {12},
    doi = {10.1016/j.inffus.2022.08.001}
}

@article{wang2021adaptive,
    title = {Adaptive Traffic Signal Control for large-scale scenario with Cooperative Group-based Multi-agent reinforcement learning},
    journal = {Transportation Research Part C: Emerging Technologies},
    volume = {125},
    pages = {103046},
    year = {2021},
    issn = {0968-090X},
    doi = {https://doi.org/10.1016/j.trc.2021.103046},
    url = {https://www.sciencedirect.com/science/article/pii/S0968090X21000760},
    author = {Tong Wang and Jiahua Cao and Azhar Hussain},
    keywords = {Multi-agent reinforcement learning, Adaptive traffic signal control, Regional green wave control, CVIS},
    abstract = {Recent research reveals that reinforcement learning can potentially perform optimal decision-making compared to traditional methods like Adaptive Traffic Signal Control (ATSC). With the development of knowledge through trial and error, the Deep Reinforcement Learning (DRL) technique shows its feasibility for the intelligent traffic lights control. However, the general DRL algorithms cannot meet the demands of agents for coordination within large complex road networks. In this article, we introduce a new Cooperative Group-Based Multi-Agent reinforcement learning-ATSC (CGB-MATSC) framework. It is based on Cooperative Vehicle Infrastructure System (CVIS) to realize effective control in the large-scale road network. We propose a CGB-MAQL algorithm that applies k-nearest-neighbor-based state representation, pheromone-based regional green-wave control mode, and spatial discounted reward to stabilize the learning convergence. Extensive experiments and ablation studies of the CGB-MAQL algorithm show its effectiveness and scalability in the synthetic road network, Monaco city and Harbin city scenarios. Results demonstrate that compared with a set of general control methods, our algorithm can better control multiple intersection cases on congestion alleviation and environmental protection.}
}


@article{essa2020selflearning,
    title = {Self-learning adaptive traffic signal control for real-time safety optimization},
    journal = {Accident Analysis \& Prevention},
    volume = {146},
    pages = {105713},
    year = {2020},
    issn = {0001-4575},
    doi = {https://doi.org/10.1016/j.aap.2020.105713},
    url = {https://www.sciencedirect.com/science/article/pii/S0001457520305388},
    author = {Mohamed Essa and Tarek Sayed},
}

@inproceedings{shiwakoti2020investigating,
    title = {Investigating the state of connected and autonomous vehicles: A literature Review},
    author = {N. Shiwakoti and P. Stasinopoulos and F. Fedele},
    booktitle = {Transportation Research Procedia},
    year = {2020},
    pages = {870-882},
    doi = {10.1016/j.trpro.2020.08.101}
}

@article{li2021planning,
    title = {Planning and Decision-making for Connected Autonomous Vehicles at Road Intersections: A Review},
    author = {S. Li and K. Shu and C. Chen and D. Cao},
    journal = {Chinese Journal of Mechanical Engineering},
    volume = {34},
    pages = {133},
    year = {2021},
    doi = {10.1186/s10033-021-00639-3}
}

@misc{gunarathna2022intelligent,
    title = {Intelligent Autonomous Intersection Management},
    author = {U. Gunarathna and S. Karunasekara and R. Borovica-Gajic and E. Tanin},
    year = {2022},
    eprint = {2202.04224},
    archivePrefix = {arXiv},
    primaryClass = {cs.RO}
}

@article{qian2017autonomous,
    author = {Qian, Xiangjun and Altché, Florent and Grégoire, Jean and de La Fortelle, Arnaud},
    title = {Autonomous Intersection Management systems: criteria, implementation and evaluation},
    journal = {IET Intelligent Transport Systems},
    volume = {11},
    number = {3},
    pages = {182-189},
    keywords = {road vehicles, road traffic control, AIM systems, autonomous vehicles, autonomous intersection management, conflicting evaluation criteria, priority-based design, intersection controller, brake-safe state},
    doi = {https://doi.org/10.1049/iet-its.2016.0043},
    url = {https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/iet-its.2016.0043},
    eprint = {https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-its.2016.0043},
    abstract = {The foreseeable large-scale deployment of autonomous vehicles in the near future raises the question of autonomous intersection management (AIM). Numerous AIM designs have been proposed, but they lack a common vision of what defines a good system. In this study, the authors discuss a set of conflicting evaluation criteria that need to be balanced in the design of an AIM system, but are often considered individually in the literature. They then introduce their own priority-based design, where an intersection controller assigns priorities to incoming vehicles. On being assigned a priority, vehicles then cross the intersection while maintaining a so-called brake-safe state with respect to higher priority vehicles, rendering the system robust. They have performed extensive simulations to showcase the properties of the proposed system, and notably that it satisfactorily balances their criteria while remaining efficient.},
    year = {2017}
}


@inproceedings{chen2019intersection,
    title = {Intersection Crossing for Autonomous Vehicles based on Deep Reinforcement Learning},
    author = {W.-L. Chen and K.-H. Lee and P.-A. Hsiung},
    booktitle = {2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)},
    year = {2019},
    pages = {1-2},
    doi = {10.1109/ICCE-TW46550.2019.8991738}
}

@book{sutton2018reinforcement,
    title = {Reinforcement Learning: An Introduction},
    author = {R. S. Sutton and A. G. Barto},
    year = {2018},
    publisher = {MIT Press}
}

@misc{johanson2021emergent,
    title = {Emergent Social Learning via Multi-agent Reinforcement Learning},
    author = {M. B. Johanson and E. Hughes and F. Timbers and J. Z. Leibo},
    year = {2021},
    eprint = {2105.06760},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{tang2020towards,
    title = {Towards Learning Multi-agent Negotiations via Self-Play},
    author = {Y. C. Tang},
    year = {2020},
    eprint = {2001.10208},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{oroojlooyjadid2019review,
    title = {A Review of Cooperative Multi-Agent Deep Reinforcement Learning},
    author = {A. OroojlooyJadid and D. Hajinezhad},
    year = {2019},
    eprint = {1908.03963},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@book{siekmann1998agent,
    title = {Agent and Multi-Agent Systems: Technologies and Applications},
    author = {J. Siekmann and W. Wahlster},
    year = {1998},
    publisher = {Springer}
}

@misc{zhou2020smarts,
    title = {SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving},
    author = {M. Zhou and et al.},
    year = {2020},
    eprint = {2010.09776},
    archivePrefix = {arXiv},
    primaryClass = {cs.AI}
}

@misc{zhang2019multiagent,
    title = {Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
    author = {K. Zhang and Z. Yang and T. Başar},
    year = {2019},
    eprint = {1911.10635},
    archivePrefix = {arXiv},
    primaryClass = {cs.MA}
}

@misc{baker2019emergent,
    title = {Emergent Tool Use From Multi-Agent Autocurricula},
    author = {B. Baker and et al.},
    year = {2019},
    eprint = {1909.07528},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{johanson2022emergent,
    title = {Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning},
    author = {M. B. Johanson and E. Hughes and F. Timbers and J. Z. Leibo},
    year = {2022},
    eprint = {2205.06760},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{baby2022analysis,
    title = {Analysis of Emergent Behavior in Multi Agent Environments using Deep Reinforcement Learning},
    author = {S. A. Baby and L. Li and A. Pokle},
    year = {2022},
    eprint = {},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@article{mnih2015humanlevel,
    title = {Human-level control through deep reinforcement learning},
    author = {V. Mnih and et al.},
    journal = {Nature},
    volume = {518},
    number = {7540},
    pages = {529-533},
    year = {2015},
    doi = {10.1038/nature14236}
}

@misc{paszke2019pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {A. Paszke and et al.},
    year = {2019},
    eprint = {1912.01703},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{li2021metadrive,
    title = {MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning},
    author = {Quanyi Li and Zhenghao Peng and Lan Feng and Qihang Zhang and Zhenghai Xue and Bolei Zhou},
    year = {2022},
    eprint = {2109.12674},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{brockman2016openai,
    title = {OpenAI Gym},
    author = {G. Brockman and et al.},
    year = {2016},
    eprint = {1606.01540},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}

@misc{bock2019ind,
    title = {The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections},
    author = {J. Bock and R. Krajewski and T. Moers and S. Runde and L. Vater and L. Eckstein},
    year = {2019},
    eprint = {1911.07602},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}

@article{Zhou2022,
    author = {Zhou, Wei and Chen, Dong and Yan, Jun and Li, Zhaojian and Yin, Huilin and Ge, Wanchen},
    title = {Multi-agent reinforcement learning for cooperative lane changing of connected and autonomous vehicles in mixed traffic},
    journal = {Autonomous Intelligent Systems},
    volume = {2},
    number = {1},
    pages = {5},
    year = {2022},
    doi = {10.1007/s43684-022-00023-5},
    url = {https://doi.org/10.1007/s43684-022-00023-5},
    issn = {2730-616X},
    abstract = {Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL) has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision-making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic (MA2C) method is proposed with a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is designed to incorporate fuel efficiency, driving comfort, and the safety of autonomous driving. A comprehensive experimental study is made that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety, and driver comfort.}
}

@article{Wang2022,
    title = {Generating merging strategies for connected autonomous vehicles based on spatiotemporal information extraction module and deep reinforcement learning},
    journal = {Physica A: Statistical Mechanics and its Applications},
    volume = {607},
    pages = {128172},
    year = {2022},
    issn = {0378-4371},
    doi = {https://doi.org/10.1016/j.physa.2022.128172},
    url = {https://www.sciencedirect.com/science/article/pii/S0378437122007300},
    author = {Shuo Wang and Hideki Fujii and Shinobu Yoshimura},
    keywords = {Connected and autonomous vehicle, Mixed traffic flow system, Deep reinforcement learning, Long–short term memory neural network, Graph convolution network, Cooperative merging},
    abstract = {A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs. Deep reinforcement learning (DRL) is a promising approach to address this challenge. It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need compliance of HDVs. However, the fusion of CAV sensing information is non-trivial. Traditional DRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment. Aimed at tackling these issues, this study proposes a DRL framework named spatiotemporal deep Q network (STDQN), by integrating a double deep Q network (DDQN) and a spatiotemporal information extraction module. A long–short term memory neural network with an attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information. In addition, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as the connectivity of multiple CAVs in a global range. Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems. Experimental results prove that as compared to baseline DRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort. The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving.}
}

@INPROCEEDINGS{Isele2017,
    author = {Isele, David and Rahimi, Reza and Cosgun, Akansel and Subramanian, Kaushik and Fujimura, Kikuo},
    booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
    title = {Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning},
    year = {2018},
    volume = {},
    number = {},
    pages = {2034-2039},
    doi = {10.1109/ICRA.2018.8461233}
}

@misc{Mnih2013,
    title = {Playing Atari with Deep Reinforcement Learning},
    author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year = {2013},
    eprint = {1312.5602},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
