@misc{chen2020interpretable,
   abstract = {Unlike popular modularized framework, end-to-end autonomous driving seeks to solve the perception, decision and control problems in an integrated way, which can be more adapting to new scenarios and easier to generalize at scale. However, existing end-to-end approaches are often lack of interpretability, and can only deal with simple driving tasks like lane keeping. In this paper, we propose an interpretable deep reinforcement learning method for end-to-end autonomous driving, which is able to handle complex urban scenarios. A sequential latent environment model is introduced and learned jointly with the reinforcement learning process. With this latent model, a semantic birdeye mask can be generated, which is enforced to connect with a certain intermediate property in today's modularized framework for the purpose of explaining the behaviors of learned policy. The latent space also significantly reduces the sample complexity of reinforcement learning. Comparison tests with a simulated autonomous car in CARLA show that the performance of our method in urban scenarios with crowded surrounding vehicles dominates many baselines including DQN, DDPG, TD3 and SAC. Moreover, through masked outputs, the learned policy is able to provide a better explanation of how the car reasons about the driving environment. The codes and videos of this work are available at our github repo and project website.},
   author = {Jianyu Chen and Shengbo Eben Li and Masayoshi Tomizuka},
   keywords = {Deep reinforcement learn-ing,End-to-end driving policy,Index Terms-Autonomous driving,Interpretability,Probabilistic graphical model},
   month = {1},
   title = {Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning},
   url = {http://arxiv.org/abs/2001.08726},
   year = {2020},
}

@article{peng2021end,
   abstract = {<p> Recent years have seen the rapid development of autonomous driving systems, which are typically designed in a hierarchical architecture or an end-to-end architecture. The hierarchical architecture is always complicated and hard to design, while the end-to-end architecture is more promising due to its simple structure. This paper puts forward an end-to-end autonomous driving method through a deep reinforcement learning algorithm Dueling Double Deep Q-Network, making it possible for the vehicle to learn end-to-end driving by itself. This paper firstly proposes an architecture for the end-to-end lane-keeping task. Unlike the traditional image-only state space, the presented state space is composed of both camera images and vehicle motion information. Then corresponding dueling neural network structure is introduced, which reduces the variance and improves sampling efficiency. Thirdly, the proposed method is applied to The Open Racing Car Simulator (TORCS) to demonstrate its great performance, where it surpasses human drivers. Finally, the saliency map of the neural network is visualized, which indicates the trained network drives by observing the lane lines. A video for the presented work is available online, <ext-link ext-link-type="uri" href="https://youtu.be/76ciJmIHMD8">https://youtu.be/76ciJmIHMD8</ext-link> or <ext-link ext-link-type="uri" href="https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html">https://v.youku.com/v_show/id_XNDM4ODc0MTM4NA==.html</ext-link> . </p>},
   author = {Baiyu Peng and Qi Sun and Shengbo Eben Li and Dongsuk Kum and Yuming Yin and Junqing Wei and Tianyu Gu},
   doi = {10.1007/s42154-021-00151-3},
   issn = {2096-4250},
   issue = {3},
   journal = {Automotive Innovation},
   keywords = {Deep Q-network,End-to-end autonomous driving,Neural network,Reinforcement learning},
   month = {8},
   pages = {328-337},
   publisher = {Springer Science and Business Media B.V.},
   title = {End-to-End Autonomous Driving Through Dueling Double Deep Q-Network},
   volume = {4},
   url = {https://link.springer.com/10.1007/s42154-021-00151-3},
   year = {2021},
}

@misc{capasso2021endtoend,
    title={End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning},
    author={A. P. Capasso and P. Maramotti and A. Dell’Eva and A. Broggi},
    year={2021},
    eprint={2104.13617},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{xu2016endtoend,
    title={End-to-end Learning of Driving Models from Large-scale Video Datasets},
    author={H. Xu and Y. Gao and F. Yu and T. Darrell},
    year={2016},
    eprint={1612.01079},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{toromanoff2019endtoend,
    title={End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances},
    author={M. Toromanoff and E. Wirbel and F. Moutarde},
    year={2019},
    archivePrefix={arXiv},
    eprint={1911.10868},
    primaryClass={cs.RO}
}

@misc{mnih2016asynchronous,
    title={Asynchronous Methods for Deep Reinforcement Learning},
    author={V. Mnih and et al.},
    year={2016},
    eprint={1602.01783},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{schulman2017proximal,
    title={Proximal Policy Optimization Algorithms},
    author={J. Schulman and F. Wolski and P. Dhariwal and A. Radford and O. Klimov},
    year={2017},
    eprint={1707.06347},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{schulman2015trust,
    title={Trust Region Policy Optimization},
    author={J. Schulman and S. Levine and P. Moritz and M. I. Jordan and P. Abbeel},
    year={2015},
    eprint={1502.05477},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{peng2021learning,
    title={Learning to Simulate Self-Driven Particles System with Coordinated Policy Optimization},
    author={Z. Peng and Q. Li and K. M. Hui and C. Liu and B. Zhou},
    year={2021},
    eprint={2110.13827},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{yang2018mean,
    title={Mean Field Multi-Agent Reinforcement Learning},
    author={Y. Yang and R. Luo and M. Li and M. Zhou and W. Zhang and J. Wang},
    year={2018},
    eprint={1802.05438},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{terry2020pettingzoo,
    title={PettingZoo: Gym for Multi-Agent Reinforcement Learning},
    author={J. K. Terry and et al.},
    year={2020},
    eprint={2009.14471},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{wu2020cooperative,
    title={Cooperative Multiagent Deep Deterministic Policy Gradient (CoMADDPG) for Intelligent Connected Transportation with Unsignalized Intersection},
    author={T. Wu and M. Jiang and L. Zhang},
    journal={Math Probl Eng},
    volume={2020},
    year={2020},
    doi={10.1155/2020/1820527}
}

@misc{petrazzini2021proximal,
    title={Proximal Policy Optimization with Continuous Bounded Action Space via the Beta Distribution},
    author={I. G. B. Petrazzini and E. A. Antonelo},
    year={2021},
    eprint={2111.02202},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{lee2021investigating,
    title={Investigating the state of connected and autonomous vehicles: A literature Review},
    author={K. M. Lee and S. G. Subramanian and M. Crowley},
    year={2021},
    eprint={2111.01100},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@article{yang2022inductive,
    title={An inductive heterogeneous graph attention-based multi-agent deep graph infomax algorithm for adaptive traffic signal control},
    author={S. Yang and B. Yang},
    journal={Information Fusion},
    volume={88},
    pages={249-262},
    year={2022},
    month={12},
    doi={10.1016/j.inffus.2022.08.001}
}

@article{wang2021adaptive,
    title={Adaptive Traffic Signal Control for large-scale scenario with Cooperative Group-based Multi-agent reinforcement learning},
    author={T. Wang and J. Cao and A. Hussain},
    journal={Transp Res Part C Emerg Technol},
    volume={125},
    year={2021},
    month={4},
    doi={10.1016/j.trc.2021.103046}
}

@article{essa2020selflearning,
    title={Self-learning adaptive traffic signal control for real-time safety optimization},
    author={M. Essa and T. Sayed},
    journal={Accid Anal Prev},
    volume={146},
    year={2020},
    month={10},
    doi={10.1016/j.aap.2020.105713}
}

@inproceedings{shiwakoti2020investigating,
    title={Investigating the state of connected and autonomous vehicles: A literature Review},
    author={N. Shiwakoti and P. Stasinopoulos and F. Fedele},
    booktitle={Transportation Research Procedia},
    year={2020},
    pages={870-882},
    doi={10.1016/j.trpro.2020.08.101}
}
@article{li2021planning,
    title={Planning and Decision-making for Connected Autonomous Vehicles at Road Intersections: A Review},
    author={S. Li and K. Shu and C. Chen and D. Cao},
    journal={Chinese Journal of Mechanical Engineering},
    volume={34},
    pages={133},
    year={2021},
    doi={10.1186/s10033-021-00639-3}
}

@misc{gunarathna2022intelligent,
    title={Intelligent Autonomous Intersection Management},
    author={U. Gunarathna and S. Karunasekara and R. Borovica-Gajic and E. Tanin},
    year={2022},
    eprint={2202.04224},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}

@inproceedings{qian2017autonomous,
    title={Autonomous Intersection Management systems: criteria, implementation and evaluation; Autonomous Intersection Management systems: criteria, implementation and evaluation},
    author={X. Qian and F. Altché and J. Grégoire and A. De and L. Fortelle},
    year={2017},
    doi={10.1049/iet-its.2016.0043}
}

@inproceedings{chen2019intersection,
    title={Intersection Crossing for Autonomous Vehicles based on Deep Reinforcement Learning},
    author={W.-L. Chen and K.-H. Lee and P.-A. Hsiung},
    booktitle={2019 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW)},
    year={2019},
    pages={1-2},
    doi={10.1109/ICCE-TW46550.2019.8991738}
}

@book{sutton2018reinforcement,
    title={Reinforcement Learning: An Introduction},
    author={R. S. Sutton and A. G. Barto},
    year={2018},
    publisher={MIT Press}
}

@misc{johanson2021emergent,
    title={Emergent Social Learning via Multi-agent Reinforcement Learning},
    author={M. B. Johanson and E. Hughes and F. Timbers and J. Z. Leibo},
    year={2021},
    eprint={2105.06760},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{tang2020towards,
    title={Towards Learning Multi-agent Negotiations via Self-Play},
    author={Y. C. Tang},
    year={2020},
    eprint={2001.10208},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{oroojlooyjadid2019review,
    title={A Review of Cooperative Multi-Agent Deep Reinforcement Learning},
    author={A. OroojlooyJadid and D. Hajinezhad},
    year={2019},
    eprint={1908.03963},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{siekmann1998agent,
    title={Agent and Multi-Agent Systems: Technologies and Applications},
    author={J. Siekmann and W. Wahlster},
    year={1998},
    publisher={Springer}
}

@misc{zhou2020smarts,
    title={SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving},
    author={M. Zhou and et al.},
    year={2020},
    eprint={2010.09776},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@misc{zhang2019multiagent,
    title={Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms},
    author={K. Zhang and Z. Yang and T. Başar},
    year={2019},
    eprint={1911.10635},
    archivePrefix={arXiv},
    primaryClass={cs.MA}
}

@misc{baker2019emergent,
    title={Emergent Tool Use From Multi-Agent Autocurricula},
    author={B. Baker and et al.},
    year={2019},
    eprint={1909.07528},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{johanson2022emergent,
    title={Emergent Bartering Behaviour in Multi-Agent Reinforcement Learning},
    author={M. B. Johanson and E. Hughes and F. Timbers and J. Z. Leibo},
    year={2022},
    eprint={2205.06760},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{baby2022analysis,
    title={Analysis of Emergent Behavior in Multi Agent Environments using Deep Reinforcement Learning},
    author={S. A. Baby and L. Li and A. Pokle},
    year={2022},
    eprint={},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{mnih2015humanlevel,
    title={Human-level control through deep reinforcement learning},
    author={V. Mnih and et al.},
    journal={Nature},
    volume={518},
    number={7540},
    pages={529-533},
    year={2015},
    doi={10.1038/nature14236}
}

@misc{paszke2019pytorch,
    title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author={A. Paszke and et al.},
    year={2019},
    eprint={1912.01703},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{li2021metadrive,
    title={MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning},
    author={Q. Li and Z. Peng and L. Feng and Q. Zhang and Z. Xue and B. Zhou},
    year={2021},
    note={Available online at: \url{https://metadriverse.github.io/metadrive}}
}

@misc{brockman2016openai,
    title={OpenAI Gym},
    author={G. Brockman and et al.},
    year={2016},
    eprint={1606.01540},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{bock2019ind,
    title={The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories at German Intersections},
    author={J. Bock and R. Krajewski and T. Moers and S. Runde and L. Vater and L. Eckstein},
    year={2019},
    eprint={1911.07602},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{Zhou2022,
   abstract = {Autonomous driving has attracted significant research interests in the past two decades as it offers many potential benefits, including releasing drivers from exhausting driving and mitigating traffic congestion, among others. Despite promising progress, lane-changing remains a great challenge for autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios. Recently, reinforcement learning (RL) has been widely explored for lane-changing decision makings in AVs with encouraging results demonstrated. However, the majority of those studies are focused on a single-vehicle setting, and lane-changing in the context of multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce attention. In this paper, we formulate the lane-changing decision-making of multiple AVs in a mixed-traffic highway environment as a multi-agent reinforcement learning (MARL) problem, where each AV makes lane-changing decisions based on the motions of both neighboring AVs and HDVs. Specifically, a multi-agent advantage actor-critic (MA2C) method is proposed with a novel local reward design and a parameter sharing scheme. In particular, a multi-objective reward function is designed to incorporate fuel efficiency, driving comfort, and the safety of autonomous driving. A comprehensive experimental study is made that our proposed MARL framework consistently outperforms several state-of-the-art benchmarks in terms of efficiency, safety, and driver comfort.},
   author = {Wei Zhou and Dong Chen and Jun Yan and Zhaojian Li and Huilin Yin and Wanchen Ge},
   doi = {10.1007/s43684-022-00023-5},
   issue = {1},
   journal = {Autonomous Intelligent Systems},
   month = {12},
   publisher = {Springer Science and Business Media LLC},
   title = {Multi-agent reinforcement learning for cooperative lane changing of connected and autonomous vehicles in mixed traffic},
   volume = {2},
   year = {2022},
}
@article{Wang2022,
   abstract = {A major challenge concerning a mixed traffic flow system, composed of connected autonomous vehicles (CAVs) and human driven vehicles (HDVs), is how to improve overall efficiency and safety by assigning appropriate control strategies to CAVs. Multi-agent deep reinforcement learning (MADRL) is a promising approach to address this challenge. It enables the joint training of multiple CAVs by fusing CAV sensing information and does not need the compliance of HDVs. However, the fusion of CAV sensing information is non-trivial. Traditional MADRL models usually fail to take advantage of connectivity among CAVs and time series characteristics of vehicle sensing information, leading to insufficient awareness of the traffic environment. Aimed at tackling these issues, this study proposes a MADRL framework integrating the double deep Q network (DDQN) and a spatiotemporal information extraction module, named spatiotemporal deep Q network (STDQN). A long-short term memory neural network with attention mechanism (AttenLSTMNN) is leveraged to extract temporal dependencies from vehicle perceptive information. Besides, a graph convolution network (GCN) is employed to model the spatial correlations among vehicles in a local range, as well as connectivity of multiple CAVs in a global range. Simulation experiments are conducted in an onramp merging scenario, which is one of the most important and commonly seen scenarios in highway or city expressway systems. Experimental results prove that as compared to baseline MADRL and rule-based methods, the proposed STDQN can improve the overall traffic efficiency, safety, and driving comfort. The proposed framework is promised to be deployed into real CAVs, to realize cooperative, safe, and efficient autonomous driving. keywords: Connected and autonomous vehicle; mixed traffic flow system; deep reinforcement learning; long-short term memory neural network; graph convolution network; cooperative merging.},
   author = {Shuo Wang and Hideki Fujii and Shinobu Yoshimura},
   doi = {10.2139/ssrn.4021924},
   issn = {1556-5068},
   journal = {SSRN Electronic Journal},
   title = {Generating Merging Strategies for Connected Autonomous Vehicles Based on Spatiotemporal Information Extraction Module and Deep Reinforcement Learning},
   url = {https://www.ssrn.com/abstract=4021924},
   year = {2022},
}
@article{Isele2017,
   abstract = {Providing an efficient strategy to navigate safely through unsignaled intersections is a difficult task that requires determining the intent of other drivers. We explore the effectiveness of Deep Reinforcement Learning to handle intersection problems. Using recent advances in Deep RL, we are able to learn policies that surpass the performance of a commonly-used heuristic approach in several metrics including task completion time and goal success rate and have limited ability to generalize. We then explore a system's ability to learn active sensing behaviors to enable navigating safely in the case of occlusions. Our analysis, provides insight into the intersection handling problem, the solutions learned by the network point out several shortcomings of current rule-based methods, and the failures of our current deep reinforcement learning system point to future research directions.},
   author = {David Isele and Reza Rahimi and Akansel Cosgun and Kaushik Subramanian and Kikuo Fujimura},
   month = {5},
   title = {Navigating Occluded Intersections with Autonomous Vehicles using Deep Reinforcement Learning},
   url = {http://arxiv.org/abs/1705.01196},
   year = {2017},
}
@article{Mnih2013,
   abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
   month = {12},
   title = {Playing Atari with Deep Reinforcement Learning},
   url = {http://arxiv.org/abs/1312.5602},
   year = {2013},
}
