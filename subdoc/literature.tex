\chapter{Literature Review}\label{ch:literature-review}
\section{Reinforcement Learning in Traffic Intersection}\label{sec:reinforcement-learning-in-traffic-intersection}
\paragraph RL has shown great promise in enabling vehicles to make intelligent decisions on the road.
RL algorithms allow autonomous vehicles to learn from their interactions with the environment, such as other vehicles, pedestrians, traffic lights, and road conditions. This learning process helps the vehicles adapt to changing situations and make appropriate decisions in real-time.

One of the significant advantages of RL in autonomous driving is its ability to handle complex and uncertain environments.
RL agents can navigate through various traffic scenarios, including intersections, merging lanes, and lane-changing maneuvers~\cite{Zhou2022, Wang2022}, by learning optimal policies based on trial and error. RL also allows autonomous vehicles to adapt to diverse driving conditions, such as different weather conditions and traffic patterns.

Applications of RL in autonomous driving include:
\begin{itemize}
    \item Trajectory Planning: RL can be used to plan safe and efficient trajectories for autonomous vehicles, considering various factors such as traffic density, speed limits, and road conditions.
    \item Behavior Prediction: RL algorithms can predict the behavior of other road users, such as pedestrians and human-driven vehicles, to anticipate their actions and plan accordingly.
    \item Lane Keeping and Lane Changing: RL can help autonomous vehicles maintain their lane position and make safe lane-changing maneuvers based on surrounding traffic.
    \item Intersection Negotiation: As seen in the context of this study, RL is applied to enable autonomous vehicles to negotiate intersections effectively, making decisions on when to yield or proceed.
    \item Optimizing Energy Consumption: RL can be used to optimize the energy consumption of autonomous vehicles, leading to longer battery life and reduced carbon emissions.
    \item Traffic Flow Optimization: RL can be employed to optimize traffic flow and reduce congestion in urban environments, improving overall transportation efficiency.
    \item Autonomous Parking: RL can be used to enable autonomous vehicles to park themselves in parking lots or on the street.
\end{itemize}
The use of RL in autonomous driving continues to evolve rapidly, with ongoing research and development focusing on improving safety, efficiency, and adaptability.
As autonomous driving technology matures, RL is expected to play a crucial role in enhancing the capabilities of autonomous vehicles and making them more reliable and intelligent on the road.
Many works try to improve transportation safety and
efficiency at the intersection by integrating various RL
techniques such as intelligent traffic light (sometimes known
as adaptive signal control)~\cite{yang2022inductive, wang2021adaptive, essa2020selflearning}.
However, as AVs are gaining ground, problems
arose as AVs had difficulty trying to recognize the state of
traffic signal in occluded situation~\cite{Isele2017}.
AVs can also rely on
other data such as Lidar points, and other sensors to navigate
through the intersection which renders the adaptive signal
control unsuitable for AVs.
Another approach was to place an
intersection controller (IC) somewhere at the center of the
intersection.
The IC relies on various data such as video
footage and lane detectors to coordinate vehicle movement.
AVs need to request passage to the IC in a similar fashion to
air traffic control~\cite{li2021planning, gunarathna2022intelligent, qian2017autonomous, chen2019intersection}.
As the solution becomes more
complex, the infrastructure cost also increased which makes
the situation only available in a big city.
RL brings promising
solutions to solve control problems in this ever-growing big-
data community.
Typically, RL algorithms are divided into 2
schools of thought â€” model-based and mode-free~\cite{sutton2018reinforcement}.
Model-based can be thought of when the agent has a complete
understanding of the environment which means the action
taken by the action guarantee the next state transition.
However, most world problems evolve round uncertainty
where agents often have incomplete view of the environment.
As agents interaction became inevitable, several MARL
algorithms were proposed~\cite{peng2021learning, yang2018mean, johanson2021emergent, tang2020towards, oroojlooyjadid2019review, siekmann1998agent, zhou2020smarts, zhang2019multiagent}.
\section{Reinforcement Learning Algorithms}\label{sec:reinforcement-learning-algorithms}

Reinforcement Learning (RL) algorithms can be broadly categorized into two main classes: model-free and model-based approaches. Each class has several sub-branches that address specific challenges and requirements in different problem domains.

\subsection{Model-Free Reinforcement Learning}\label{subsec:model-free-reinforcement-learning}
Model-free reinforcement learning (RL) is a type of reinforcement learning approach where an agent learns to make decisions and take actions in an environment without explicitly building a model of the environment. In traditional RL, an agent typically constructs a model that represents the transition dynamics and rewards of the environment. This model helps the agent to simulate and plan future actions to make decisions. However, in model-free RL, the agent directly learns from experience through trial and error without explicitly constructing a model.

In model-free RL, the agent interacts with the environment over multiple episodes or time steps, and it receives feedback in the form of rewards based on the actions it takes. The primary goal of the agent is to maximize the total accumulated reward over time. To achieve this, the agent uses different algorithms and techniques to learn the best policy, which is a mapping from states to actions that guides the agent in making decisions.

Model-free RL is particularly useful when the environment is complex, and constructing an accurate model is difficult or computationally expensive.
Instead of relying on a model, model-free RL agents learn from real interactions with the environment, making them more adaptable and applicable to a broader range of scenarios.
However, model-free RL also has some challenges. Since it does not use a model, learning can be more sample inefficient~\cite{sutton2018reinforcement}, requiring a large number of interactions with the environment to converge to an optimal policy. Additionally, model-free RL can sometimes struggle with environments that have sparse rewards or deceptive reward structures. Nevertheless, with proper tuning and algorithm selection, model-free RL can be highly effective in a wide range of real-world applications.
\subsubsection{Q-Learning}\label{subsubsec:q-learning}

Q-learning is a model-free reinforcement learning algorithm that estimates the action-value function \(Q(s, a)\), representing the expected cumulative reward when taking action \(a\) in state \(s\). It is a widely used algorithm for solving Markov Decision Processes (MDPs) without requiring a model of the environment's dynamics.

The Q-value is updated iteratively using the Bellman equation:

\[Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]\]

where:
\(Q(s, a)\) is the Q-value of state-action pair \((s, a)\),
\(r\) is the immediate reward received after taking action \(a\) in state \(s\),
\(\alpha\) is the learning rate that controls the step size of updates,
\(\gamma\) is the discount factor that balances future rewards' importance,
\(s'\) is the next state reached after taking action \(a\) in state \(s\), and
\(\max_{a'} Q(s', a')\) represents the maximum Q-value over all possible actions in state \(s'\).

The Q-learning algorithm follows the following steps:
\begin{enumerate}
    \item Initialize the Q-values for all state-action pairs to arbitrary values.
    \item Observe the current state \(s\).
    \item Select an action \(a\) using an exploration policy, such as \(\epsilon\)-greedy, which balances between exploration and exploitation.
    \item Execute the selected action \(a\) in the environment.
    \item Observe the immediate reward \(r\) and the next state \(s'\).
    \item Update the Q-value for the current state-action pair using the Bellman equation.
    \item Repeat steps 2 to 6 until convergence or a predefined number of iterations.
\end{enumerate}
Q-learning is an off-policy algorithm, meaning that it learns the optimal policy while following a different behavior policy for exploration. The exploration policy allows the agent to explore the environment and learn about different state-action pairs.
As Q-learning iteratively updates the Q-values, it converges to the optimal action-value function \(Q^*(s, a)\), which represents the maximum expected cumulative reward from each state-action pair under the optimal policy.
Q-learning is known to be model-free, which means it does not require knowledge of the environment's dynamics or transition probabilities. Instead, it learns from interacting with the environment and experiencing different state-action-reward sequences.
The Q-learning algorithm is widely used due to its simplicity and ability to handle large state spaces. However, it may suffer from slow convergence in complex environments, and variants like Double Q-learning and Deep Q-Networks (DQNs) have been introduced to address these limitations.

\subsubsection{Deep Q-Networks (DQNs)}
Deep Q-Networks (DQNs) is a deep learning extension of the Q-learning algorithm. DQNs leverage the power of deep neural networks to approximate the action-value function \(Q(s, a)\) in complex and high-dimensional environments. It was introduced as a breakthrough in combining deep learning and reinforcement learning, particularly in solving Atari 2600 games~\cite{Mnih2013}.

The core idea of DQNs is to use a neural network as a function approximator for the action-value function. The neural network takes the state \(s\) as input and outputs a vector of Q-values, one for each possible action. The action with the highest Q-value is then selected as the agent's action in the given state.

DQNs aim to solve two key challenges in Q-learning:
\begin{itemize}
    \item \textbf{Curse of Dimensionality}: In traditional tabular Q-learning, the size of the Q-table grows exponentially with the number of states and actions, making it infeasible for large state spaces. DQNs address this issue by using a neural network to generalize across states and actions.
    \item \textbf{High-Dimensional Inputs}: Many real-world environments have high-dimensional state spaces, such as images in computer vision tasks. DQNs can handle these inputs by using convolutional neural networks (CNNs) as the function approximator.
\end{itemize}

The DQN algorithm follows these main steps:

1.
\textbf{Experience Replay}: DQNs use an experience replay buffer to store transitions (state, action, reward, next state) experienced by the agent. This buffer helps decorrelate experiences and reduce the correlation between consecutive updates, leading to more stable training.

2.
\textbf{Target Network}: To stabilize the learning process, DQNs use a separate target network with fixed parameters to generate the target Q-values during the update step. The target network's parameters are updated periodically to match the primary Q-network.

3.
\textbf{Loss Function}: The DQN loss function is the mean squared error between the predicted Q-values and the target Q-values, given by the Bellman equation:

\[Loss = \frac{1}{N} \sum_{i}(Q(s_i, a_i) - (r_i + \gamma \max_{a'}Q_{\text{target}}(s'_i, a')))^2\]

where \(N\) is the batch size, \(Q_{\text{target}}\) represents the target Q-network, and \(\gamma\) is the discount factor.

The DQN algorithm iteratively samples batches of experiences from the replay buffer, computes the loss, and performs stochastic gradient descent to update the primary Q-network's parameters.

DQNs have demonstrated remarkable success in various tasks, including playing Atari games and controlling robotic systems. They can handle high-dimensional inputs and effectively approximate the action-value function in complex environments.

However, DQNs also have some challenges, such as the tendency to overestimate Q-values and difficulties in handling continuous action spaces. Several extensions, such as Double DQNs and Dueling DQNs, have been proposed to address these limitations and further improve performance.

\subsubsection{Policy Gradient Methods}
Policy Gradient Methods are a class of reinforcement learning algorithms that directly optimize the policy of an agent to maximize the expected cumulative reward. Unlike value-based methods that estimate the action-value function \(Q(s, a)\), policy gradient methods aim to learn the policy \(\pi(a|s)\), which is a mapping from states \(s\) to actions \(a\).

The policy is typically represented as a parametric function, such as a neural network, with trainable parameters \(\theta\). The objective of policy gradient methods is to find the optimal parameters \(\theta^*\) that maximize the expected reward:

\[J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \gamma^t r_t \right]\]

where \(J(\theta)\) is the objective function, \(\tau\) is a trajectory (sequence of states, actions, and rewards), \(\pi_{\theta}\) is the policy with parameters \(\theta\), \(r_t\) is the reward at time step \(t\), \(\gamma\) is the discount factor, and \(T\) is the maximum time step in the trajectory.

Policy gradient methods employ gradient ascent to update the policy parameters in the direction that increases the expected reward. The policy gradient is computed using the likelihood ratio trick:

\[\nabla_{\theta} J(\theta) = \mathbb{E}_{\tau \sim \pi_{\theta}} \left[ \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) \left( \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'} \right) \right]\]

This gradient can be estimated through sampling by interacting with the environment and collecting trajectories.
The policy parameters are then updated using stochastic gradient ascent:

\[\theta_{\text{new}} = \theta_{\text{old}} + \alpha \nabla_{\theta} J(\theta)\]

where \(\alpha\) is the learning rate.

Policy gradient methods offer several advantages, such as handling continuous action spaces and directly optimizing the policy without relying on value functions. They are also well-suited for problems with stochastic policies or environments.

However, policy gradient methods can suffer from high variance in the gradient estimates, which can lead to slow convergence. To address this, variance reduction techniques like baseline subtraction and advantage functions are used to stabilize training.

Additionally, modern policy gradient methods, such as Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO), introduce mechanisms to constrain the policy updates to ensure more stable and controlled learning.

Overall, policy gradient methods have proven to be effective in a wide range of applications, including robotic control, game playing, and natural language processing, where learning directly from high-dimensional observations is crucial.

\subsubsection{Proximal Policy Optimization (PPO)}
Proximal Policy Optimization (PPO) is a state-of-the-art policy gradient algorithm for reinforcement learning. It addresses the challenges of traditional policy gradient methods, such as high variance and instability, by introducing constraints on policy updates to ensure more stable and controlled learning.

PPO operates by iteratively optimizing a surrogate objective function that approximates the policy's performance while avoiding drastic policy changes that could lead to divergence. The objective function is defined as the clipped surrogate objective:

\[L^{CLIP}(\theta) = \mathbb{E}_{t} \left[ \min \left( \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)} A^{\pi_{\theta_{\text{old}}}}(s_t, a_t), \text{clip}\left( \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon \right) A^{\pi_{\theta_{\text{old}}}}(s_t, a_t) \right) \right]\]

where:
\(\theta\) represents the current policy parameters,
\(\theta_{\text{old}}\) represents the old policy parameters (used as a reference),
\(\pi_{\theta}(a_t|s_t)\) is the probability of taking action \(a_t\) in state \(s_t\) under the current policy,
\(\pi_{\theta_{\text{old}}}(a_t|s_t)\) is the probability of taking action \(a_t\) in state \(s_t\) under the old policy,
\(A^{\pi_{\theta_{\text{old}}}}(s_t, a_t)\) is the advantage function, representing the advantage of taking action \(a_t\) in state \(s_t\) under the old policy,
\(\epsilon\) is a hyperparameter that controls the magnitude of the policy change.

The clipped surrogate objective encourages small policy updates by limiting the change in the likelihood ratio between the new and old policies. This constraint prevents the policy from changing too rapidly, which contributes to the algorithm's stability.

The PPO algorithm operates in two phases:

1.
\textbf{Data Collection}: The agent interacts with the environment to collect trajectories using the current policy \(\pi_{\theta}\).

2.
\textbf{Policy Update}: The policy parameters \(\theta\) are updated using stochastic gradient ascent to maximize the clipped surrogate objective function \(L^{CLIP}(\theta)\).

PPO's ability to strike a balance between exploration and exploitation makes it effective in a wide range of tasks. The algorithm has been successfully applied to various domains, including robotic control, game playing, and natural language processing.

PPO also offers a robust and efficient solution for parallelizing training across multiple environments, further accelerating learning in complex environments.

Overall, Proximal Policy Optimization (PPO) has become a popular choice for deep reinforcement learning due to its stable performance, ease of implementation, and excellent results across diverse problem domains.

\subsection{Model-Based Reinforcement Learning}



Model-based reinforcement learning (RL) is an approach where an agent learns and utilizes an explicit model of the environment to make decisions and plan actions.
Unlike model-free RL, which directly learns from experience without constructing a model, model-based RL involves creating a representation of how the environment behaves, including its transition dynamics and reward function.
This model is then used to simulate possible trajectories and predict the outcomes of different actions.

The typical process of model-based RL can be summarized as follows:

\begin{itemize}
    \item \textbf{Model Learning} : The agent collects data by interacting with the environment and observes the outcomes of its actions. It uses this data to build a model of the environment. The model could be in the form of a transition function, which predicts the next state given the current state and action, and a reward function, which estimates the expected reward for a state-action pair.
    \item \textbf{Planning} : The agent uses its model to simulate possible trajectories and predict the outcomes of different actions. It can perform offline planning to evaluate different sequences of actions and predict the total expected rewards. This process allows the agent to make decisions that are based on the simulations and predictions from the model, rather than relying solely on trial and error in the real environment.
    \item \textbf{Policy Improvement} : After simulating different trajectories and estimating the rewards, the agent can update its policy to improve decision-making. It can use various optimization algorithms to find a policy that maximizes the expected cumulative reward based on the predictions from the model.
\end{itemize}

Model-based RL has several advantages:

\begin{itemize}
    \item \textbf{Sample Efficiency} : Since the agent can simulate interactions with the environment using its model, it can potentially learn more efficiently and require fewer real interactions to find an optimal policy.
    \item \textbf{Safety} : The agent can use its model to explore the environment safely, avoiding potentially harmful or risky actions by simulating their consequences.
    \item \textbf{Handling Partial Observability} : Model-based RL can be effective in partially observable environments where the agent does not have access to complete information about the state. The model helps the agent predict the underlying state, improving decision-making.
\end{itemize}

However, model-based RL also has its challenges:

\begin{itemize}
    \item \textbf{Model Accuracy} : The success of model-based RL heavily depends on the accuracy of the learned model. If the model is inaccurate, the agent's decisions and predictions can be suboptimal.
    \item \textbf{Model Complexity} : Constructing an accurate model may be computationally expensive or infeasible in complex environments with large state spaces.
    \item \textbf{Planning Overhead} : Planning using the model can introduce additional computational overhead, which might limit the real-time applicability of the approach in certain scenarios.
\end{itemize}

\subsubsection{Dynamic Programming}


Dynamic programming is a powerful optimization technique used in computer science and mathematics to solve problems by breaking them down into smaller subproblems and solving each subproblem only once. The solutions to these subproblems are stored and reused when needed, which avoids redundant calculations and leads to more efficient computations.

The fundamental idea behind dynamic programming is to divide a complex problem into smaller, overlapping subproblems and solve each subproblem just once, storing its solution in a table or array. Then, the solution to the original problem is constructed by combining the solutions of the subproblems.

The process of dynamic programming can be described by the following steps:

\begin{itemize}
    \item \textbf{Define the Recurrence Relation} : Express the solution to a larger problem in terms of the solutions to its subproblems. This recursive relationship defines the optimal value of the larger problem based on the optimal values of its subproblems.
    \item \textbf{Identify Overlapping Subproblems} : Analyze the problem to identify if there are any overlapping subproblems, i.e., if the same subproblems are solved multiple times in the computation. This is the key property that makes dynamic programming effective, as we can avoid redundant computations.
    \item \textbf{Create a Memoization Table} : Use a data structure, such as an array or a dictionary, to store the solutions of the subproblems as they are computed. This is called memoization, and it helps in efficiently retrieving solutions for previously solved subproblems.
    \item \textbf{Bottom-Up or Top-Down Approach} : Dynamic programming can be implemented using either a bottom-up or top-down approach.
    \begin{itemize}
        \item \textbf{Bottom-Up} : In the bottom-up approach, we start solving the smallest subproblems first and gradually build up to the solution of the original problem.
        \item \textbf{Top-Down} : In the top-down approach, also known as memoization, we start with the original problem and break it down into smaller subproblems.
    \end{itemize}
\end{itemize}

Dynamic programming is widely used in various areas of computer science, such as algorithm design, optimization problems, artificial intelligence, and bioinformatics, to efficiently solve complex problems with overlapping subproblems.

\subsubsection{Monte Carlo Tree Search (MCTS)}
Monte Carlo Tree Search (MCTS) is a popular algorithm used in decision-making and game playing problems, particularly in games with large state spaces and complex branching factors. It is commonly applied in artificial intelligence for games like Go, Chess, and other strategic board games. MCTS combines elements of random sampling (Monte Carlo) with tree-based search to efficiently explore the game tree and make informed decisions.

The MCTS algorithm can be explained in the following steps:
\begin{itemize}
    \item \textbf{Selection} : Starting from the root of the search tree, the algorithm traverses the tree by selecting child nodes based on a selection policy, often using Upper Confidence Bound (UCB) or variants like UCT (Upper Confidence Bounds for Trees). The selection policy balances between exploring new nodes and exploiting existing knowledge to focus on promising branches.
    \item \textbf{Expansion} : Once a leaf node (a node with unexplored children) is reached, the algorithm expands the tree by adding child nodes representing possible game moves or actions.
    \item \textbf{Simulation (Rollout)} : From the newly added child node, the algorithm performs a Monte Carlo simulation (also known as a rollout) by playing the game randomly until a terminal state (end of the game) is reached. The result of the simulation is a reward or utility value representing the outcome of the game from that state.
    \item \textbf{Backpropagation} : The reward obtained from the simulation is backpropagated up the tree, updating the statistics of each visited node, such as the total reward and the number of visits. This information helps in refining the selection policy and improves the decision-making.
    \item \textbf{Repeated Iteration}: Steps 1 to 4 are repeated for a certain number of iterations or until a predefined computational budget is reached. The more iterations performed, the more refined the search tree becomes, leading to better decisions.
\end{itemize}


\subsubsection{Model Predictive Control (MPC)}
Model Predictive Control (MPC) is an advanced control strategy used in various engineering fields, such as process control, robotics, and automotive systems. MPC is particularly well-suited for systems with complex dynamics, constraints, and uncertainties. It involves using a model of the system's behavior to predict future states and optimize a control sequence over a finite time horizon. The control sequence is applied to the system, and the process is iterated in a receding horizon fashion, continually updating the control actions based on the most recent measurements and predictions.

Here's a detailed explanation of Model Predictive Control:
\begin{itemize}
    \item \textbf{Modeling the System} : The first step in MPC is to create a dynamic model of the system being controlled. This model should accurately represent the system's behavior and dynamics. Depending on the complexity of the system and the available information, the model can be a physics-based model, an empirical model, or even a data-driven model based on machine learning techniques.
    \item \textbf{Prediction Horizon} : MPC works by optimizing a control sequence over a finite prediction horizon, which is a predetermined time interval into the future. The length of the prediction horizon determines how far into the future the control actions are computed. Longer prediction horizons can provide better performance, but they also increase the computational burden.
    \item \textbf{Optimization Objective} : The main objective of MPC is to optimize a cost function over the prediction horizon. The cost function is a measure of how well the system performs, and it typically includes components related to control effort, desired setpoints, and constraints. By minimizing or maximizing the cost function, MPC finds the optimal control sequence that achieves the desired control objectives.
    \item \textbf{Constraints} : MPC can handle both hard and soft constraints on the system's states and inputs. Hard constraints are strict limits that cannot be violated, while soft constraints are desired limits that can be violated if necessary. MPC can also handle probabilistic constraints, which are constraints that must be satisfied with a certain probability.
    \item \textbf{Receding Horizon Control} : Once the optimal control sequence is computed, only the first control action is applied to the system. The process is then repeated, with the optimization being performed again using the most recent measurements and predictions. This is called receding horizon control, as the optimization horizon keeps receding into the future.
    \item \textbf{Real-Time Implementation} : MPC is typically implemented in real-time using a digital computer. The optimization problem is solved at each time step, and the first control action is applied to the system. The process is repeated at the next time step, and so on. The real-time implementation of MPC requires fast and efficient algorithms to solve the optimization problem.
    \item \textbf{Robustness and Adaptation} : MPC is inherently robust to disturbances and uncertainties, as it continually updates the control actions based on the most recent measurements and predictions. MPC can also be extended to handle model uncertainties and unmodeled dynamics by using adaptive MPC techniques.
\end{itemize}

MPC is widely used in industrial automation, autonomous systems, power systems, and various other applications where precise control and handling of complex dynamics and constraints are essential. It provides robust and adaptive control, making it a valuable tool in controlling complex systems in real-world scenarios.

%\subsection{Hybrid Approaches}
%
%Hybrid RL algorithms combine elements of both model-free and model-based methods to exploit their respective advantages.
%
%\subsubsection{Model-Based Reinforcement Learning with Value Functions}
%Some approaches use a learned value function in combination with a model to improve sample efficiency and planning.
%
%\subsubsection{Model-Free Methods with Model-Based Initialization}
%In some cases, model-free algorithms are initialized with a learned model, which helps speed up convergence.
\section{Single Agent Reinforcement Learning}\label{sec:single-agent-reinforcement-learning}
\paragraph Single-agent RL algorithms are designed to solve problems where there is only one agent interacting with the environment.
There are advantages and disadvantages to using single-agent RL algorithms in autonomous driving applications:
\begin{itemize}
    \item Simplicity: Single-agent RL is relatively straightforward to implement and understand since there is only one agent interacting with the environment.
    \item Model-Based Approaches: Model-based RL algorithms can work effectively in environments where the agent has a complete understanding of the environment's dynamics, enabling accurate predictions.
    \item Sample Efficiency: Single-agent RL algorithms can be more sample-efficient compared to MARL algorithms since they do not involve interactions among multiple agents.
\end{itemize}
However, as the number of agents increases, single-agent RL algorithms may struggle to scale effectively, especially when the agents' actions affect each other.
So we have some disadvantages:
\begin{itemize}
    \item Scalability: In complex environments with many agents, single-agent RL algorithms may struggle to scale effectively, especially when the agents' actions affect each other.
    \item Non-Stationary Environments: Single-agent RL assumes a stationary environment, which may not hold true in dynamic settings with multiple agents.
\end{itemize}
\section{Multi-Agent Reinforcement Learning}\label{sec:multi-agent-reinforcement-learning}
The advantages of MARL in autonomous driving include:
\begin{itemize}
    \item Coordination and Cooperation: MARL allows agents to coordinate and cooperate to achieve common goals or address conflicting objectives, making it suitable for problems with multiple agents interacting.
    \item Dynamic Environments: MARL is well-suited for non-stationary and complex environments, where the behavior of agents affects each other and evolves over time.
    \item Flexibility: MARL algorithms can handle various scenarios, from collaborative tasks to competitive interactions, making them versatile in real-world settings.
\end{itemize}
However, MARL also has some disadvantages:
\begin{itemize}
    \item Curse of Dimensionality: As the number of agents increases, the state space grows exponentially, leading to challenges in scalability and computation.
    \item Communication Overhead: In some MARL algorithms, communication among agents may be required to coordinate actions, introducing additional overhead and complexity.
    \item Partial Observability: In scenarios where agents have limited visibility of the environment or other agents' states, coordinating effectively becomes more challenging.
\end{itemize}