\section{Preliminary}\label{sec:preliminary}
\subsection{Markov Decision Process}\label{subsec:markov-decision-process}
\paragraph A Markov decision process (MDP) is a tuple $(S, A, P, R, \gamma)$, where: $S$ is a set of states. $A$ is a set of actions. $P(s'|s, a)$ is the probability of transitioning from state $s$ to state $s'$ when action $a$ is taken.
$R(s, a)$ is the reward received when taking action $a$ in state $s$. $\gamma \in [0, 1)$ is a discount factor that determines the importance of future rewards.
The MDP definition can be interpreted as follows:
\begin{itemize}
    \item The state $s$ represents the current state of the environment.
    \item The action $a$ represents the agent's decision in the current state.
    \item The transition probability $P(s'|s, a)$ represents the probability of the environment transitioning to state $s'$ after the agent takes action $a$ in state $s$.
    \item The reward $R(s, a)$ represents the amount of reward the agent receives after taking action $a$ in state $s$.
    \item The discount factor $\gamma$ represents how much the agent values future rewards relative to immediate rewards.
\end{itemize}
Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are a class of multi-agent reinforcement learning problems where multiple agents collectively aim to maximize a global objective function while facing uncertainty in a partially observable environment.

\subsubsection*{Elements of Dec-POMDPs:}
\begin{enumerate}
    \item \textbf{Agents:} Dec-POMDPs involve multiple agents denoted by index $i \in \{1, 2, \ldots, N\}$, each with its own local observation space $O_i$ and action set $A_i$. The agents interact with the environment and communicate with each other to make coordinated decisions.

    \item \textbf{States:} The environment's underlying state is not directly observable. Instead, each agent $i$ receives partial, private observations $o_i$ that provide probabilistic information about the true state. The state space is denoted as $S$.

    \item \textbf{Observations:} Each agent's observation $o_i$ is a partial and noisy representation of the global state. Observations are generated based on the current state $s$ and the agent's local observation function denoted as $O_i(s)$.

    \item \textbf{Actions:} Agents take actions $a_i$ based on their local observations and communication with other agents. The joint action of all agents affects the evolution of the underlying state. The joint action of all agents is denoted as $\mathbf{a} = (a_1, a_2, \ldots, a_N)$.

    \item \textbf{Transition Model:} The transition model $P(s' | s, \mathbf{a})$ describes the probability distribution over next states $s'$ given the current state $s$ and joint actions of all agents $\mathbf{a}$.

    \item \textbf{Observation Model:} The observation model $P(o_i | s)$ describes the probability distribution over observations $o_i$ given the current state $s$ for agent $i$.

    \item \textbf{Global Objective Function:} The goal of the agents is to optimize a global objective function $J(\mathbf{a})$ that captures the overall mission or task. This objective function depends on the collective actions of all agents and may involve cooperation and coordination.

\end{enumerate}
\subsection{Gaussian Distribution}\label{subsec:gaussian-distribution}
The Gaussian distribution, also known as the normal distribution, is a continuous probability distribution widely used to model random variables in various real-world scenarios. It is characterized by its mean $\mu$ and variance $\sigma^2$. The probability density function (PDF) of a Gaussian distribution is given by:

\begin{equation}
    f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}},\label{eq:equation}
\end{equation}

where $x$ is a random variable, $\mu$ is the mean (expected value), and $\sigma^2$ is the variance.

The Gaussian distribution is symmetric around its mean, and its shape is determined by the variance. When the variance is small, the distribution is more concentrated around the mean, while a larger variance leads to a wider distribution.
\begin{figure}
    \centering
    \includegraphics[width=12cm]{assets/normal}
    \caption{Normal Distribution}\label{fig:figure}
\end{figure}

\subsection*{Policy Gradient Method with Gaussian Approximation:}
In Reinforcement Learning (RL), the policy gradient method is a popular approach to learning the policy of an agent in a continuous action space. The policy gradient method directly optimizes the policy parameters to maximize the expected cumulative reward.

\subsubsection*{Parameterized Policy:}
To handle continuous actions, the policy is often parameterized as a Gaussian distribution with a mean vector $\boldsymbol{\mu}$ and a variance vector $\boldsymbol{\sigma}^2$, or alternatively, using the standard deviation $\boldsymbol{\sigma}$. The policy function is defined as:

\begin{equation}
    \pi_{\boldsymbol{\theta}}(a | s) = \mathcal{N}(\boldsymbol{\mu}(s; \boldsymbol{\theta}), \text{diag}(\boldsymbol{\sigma}(s; \boldsymbol{\theta}))),\label{eq:equation2}
\end{equation}

where $\pi_{\boldsymbol{\theta}}(a | s)$ is the policy function with parameters $\boldsymbol{\theta}$, and $\text{diag}(\cdot)$ extracts the diagonal elements to form a diagonal covariance matrix.

\subsubsection*{Prediction of Mean Using Neural Network:}
In practice, the mean $\boldsymbol{\mu}(s; \boldsymbol{\theta})$ is often predicted using a neural network with state $s$ as input and the policy parameters $\boldsymbol{\theta}$ as weights.
The neural network learns to map states to action means, allowing the agent to approximate actions based on observations from the environment.

\subsubsection*{Adjusting Standard Deviation Over Time:}
During the training process, the standard deviation $\boldsymbol{\sigma}(s; \boldsymbol{\theta})$ can be adjusted over time.
It is often initialized with some initial value and then decayed towards a minimum value.
This adjustment allows the agent to explore more at the beginning of training (high uncertainty) and become more confident in its actions as training progresses (low uncertainty).

\subsection{Beta Distribution}\label{subsec:beta-distribution}
The Beta distribution is a continuous probability distribution defined on the interval [0, 1].
It is commonly used to model random variables that represent probabilities or proportions.
The Beta distribution is parameterized by two shape parameters, denoted as $\alpha$ and $\beta$, which control the shape of the distribution. The probability density function (PDF) of the Beta distribution is given by:

\begin{equation}
    f(x | \alpha, \beta) = \frac{1}{B(\alpha, \beta)} x^{\alpha - 1}(1 - x)^{\beta - 1},\label{eq:equation4}
\end{equation}

where $x$ is a random variable, $\alpha$ and $\beta$ are the shape parameters, and $B(\alpha, \beta)$ is the Beta function:

\begin{equation}
    B(\alpha, \beta) = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha + \beta)},\label{eq:equation3}
\end{equation}

where $\Gamma(\cdot)$ is the gamma function.

The Beta distribution is asymmetric and its shape is determined by the values of $\alpha$ and $\beta$. When both $\alpha$ and $\beta$ are greater than 1, the distribution is unimodal and skewed.
The mode of the distribution is at $\frac{\alpha - 1}{\alpha + \beta - 2}$.

\begin{figure}
    \centering
    \includegraphics[width=12cm]{assets/beta}
    \caption{Beta Distribution}\label{fig:figure2}
\end{figure}

\subsection*{Policy Gradient Method with Beta Distribution:}
In the policy gradient method for Reinforcement Learning (RL), the policy is often parameterized as a Beta distribution to handle continuous actions within the range [0, 1].

\subsubsection*{Predicting Alpha and Beta Values from Neural Network:}
To approximate the Beta distribution for policy generation, a neural network is used to predict the $\alpha$ and $\beta$ parameters.
The neural network takes the state $s$ as input and produces the $\alpha$ and $\beta$ values as output.
These values are then used to define the Beta distribution for sampling actions.

\subsubsection*{Appropriate Mapping for Action Space:}
Because the accepted action space is within [0, 1], the sampled values from the Beta distribution need to be mapped to this interval.
This can be achieved by directly using the sampled values as the actions.

\subsubsection*{Interpretation of 1 - Alpha and 1 - Beta:}
The $\alpha$ and $\beta$ values can be interpreted as counts of successes and failures, respectively.
Specifically, if $\alpha > 1$ and $\beta > 1$, then $\alpha - 1$ and $\beta - 1$ can be thought of as the number of successes and failures, respectively.
This interpretation is useful when modeling probabilities or proportions in the context of the policy gradient method.

\section{Beta Policy}\label{sec:beta-policy}
\paragraph We propose a straightforward neural network architecture, specifically a Multilayer Perceptron (MLP), with 2 hidden layers each having a size of 256 neurons. This neural network is then divided into two separate channels, each having a hidden layer with 256 neurons. These channels are designed to predict values for steering angle and acceleration, respectively, in order to control the vehicle.

The implementation of the neural network is carried out using PyTorch~\cite{paszke2019pytorch}, a popular deep learning library. The input to the network consists of ego states, which include relevant information about the vehicle in relation to its environment, such as velocity, checkpoint positions, time, and distance to the destination. Additionally, LiDAR data from the left, right, and front sides of the vehicle is collected and fed into the network. LiDAR is a sensing technology used in many autonomous vehicles, and its utilization allows the proposed algorithm to be easily adapted and applied in real-world scenarios.

The proposed neural network architecture employs an MLP with two hidden layers, each with 256 neurons. It is divided into two channels for predicting steering angle and acceleration values, respectively. The input includes ego states and LiDAR data, making it suitable for controlling the vehicle in an autonomous driving context. The implementation is carried out using PyTorch, a popular deep learning framework known for its efficiency and flexibility.
\begin{figure}
    \centering
    \includegraphics[width=12cm]{assets/beta_policy}
    \caption{Beta Policy}\label{fig:figure3}
\end{figure}

The final layer that output these values uses softplus
activation function and 1 is added to make sure that the output
is always positive. The final layer is $f(x) = 1 + \log (1 +
e^x)$ . As the environment accepts action with $a = [sa,acc]^T=[-1,1]^2$, the output from Beta's PDF is then mapped with the following equation:
\begin{equation}
    sa = 2h(\beta_{sa}, \alpha_{sa}) - 1\label{eq:equation5}
\end{equation}
\begin{equation}
    acc = 2h(\beta_{acc}, \alpha_{acc}) - 1\label{eq:equation6}
\end{equation}

where $h(\beta, \alpha)$ is the mean of the Beta distribution with parameters $\beta$ and $\alpha$, $sa$ stands for steering angle, and $acc$ stands for acceleration.
It should be noted that we only consider the case where $\alpha > 1$ and $\beta > 1$.